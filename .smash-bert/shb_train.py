# -*- coding: utf-8 -*-
"""Siamese-Hier-BERT.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AO--T3X0sDMgQB3Rj5NSbO-rA9biBAz6
"""


"""## Imports"""

from tqdm import tqdm
import torch
import torch.nn as nn
import torch.nn.functional as F
import spacy
from collections import defaultdict
import sys
from torch.optim import AdamW
import torch.nn.utils.rnn as U
nlp = spacy.load("en_core_web_sm")
import json
from transformers import DistilBertTokenizer, DistilBertModel
import os

bert_variant = os.environ['SHB_BERT_VARIANT']
bert_tokenizer = DistilBertTokenizer.from_pretrained(bert_variant)

NUMPATH  = f"{os.environ['ELECTER_HULK_DIR']}/{os.environ['LEGAL_DATA_DIR']}/SHB/{os.environ['SHB_BERT_VARIANT']}"
os.system(f"mkdir -p {NUMPATH}")
filen    = f"{NUMPATH}/pre-processed-data.json"
SAVEPATH = f"{NUMPATH}/output/"

"""## Model Hyperparameters"""

EMBEDDING_DIM = 768
BATCH_SIZE = 1
LR = 1e-5
L2REG = 0.00001
DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'
NUM_EPOCHS = 30
DEV_SPLIT = 0.2

"""# Setup for experiments

### Load numericalized data
"""
ct = 0
with open(filen) as fr:
    traindev_num = json.load(fr)
#traindev_num = traindev_num[:10]
#with open(NUMPATH + "test.json") as fr:
 #   test_num = json.load(fr)


"""## Model"""

class BiLSTMAttn(nn.Module):
    def __init__(self, embedding_dim, dropout = 0.1):
        super().__init__()
        
        self.embedding_dim = embedding_dim
        
        self.rnn = nn.GRU(embedding_dim, embedding_dim // 2, bidirectional = True, batch_first = True)
        self.fc = nn.Linear(embedding_dim, embedding_dim)
        
        self.context = nn.Parameter(torch.rand(embedding_dim, ))
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, flat_input, input_lens): # flat_input: [S',E'], input_lens: [B']        
        batch_size = input_lens.shape[0]
        assert flat_input.shape[0] == torch.sum(input_lens), "Mismatch of sequences"
        
        # pack sequence
        packed_input = U.pack_sequence(torch.split(flat_input, input_lens.tolist()), enforce_sorted = False)
        
        # pass sequence through RNN
        packed_output, _ = self.rnn(packed_input) # [S',E'] --> [S',E']
        output = U.pad_packed_sequence(packed_output, batch_first = True)[0] # [S',E'] --> [B',s',E']
        
        activated_output = self.dropout(torch.tanh(self.fc(output))) # [B',s',E'] --> [B',s',E']
        
        # calculate attention probabilities
        logits = torch.bmm(activated_output, self.context.expand(batch_size, self.embedding_dim).unsqueeze(2)) # [B',s',E'], [B',E',1] --> [B',s',1]
        # [B',s']
        input_mask = (torch.arange(input_lens.max(), device=input_lens.device).expand(batch_size, input_lens.max()) < input_lens.unsqueeze(1)).float()
        logits[input_mask == 0] = -9999
        probs = F.softmax(logits, dim = 1) 
        
        #print(probs.shape, output.shape)
        
        # calculate final representation
        weighted_output = torch.sum(probs * output, dim = 1).squeeze(1) # [B',s',1], [B',s',E'] --> [B',E']
        return weighted_output

class DummyLayer(nn.Module):
    def __init__(self):
        super().__init__()
        self.dummy = nn.Parameter(torch.ones(1, dtype = torch.float))
    def forward(self,x):
        return x + self.dummy - self.dummy

class MashRNN(nn.Module):
    def __init__(self, E, device = DEVICE):
        super().__init__()
        
        self.embedding_dim = E
        self.device = device
        
        # word-level encoder
        #self.word_encoder = BiLSTMAttn(self.embedding_dim)
        
        # sentence-level encoder
        self.sent_encoder = BiLSTMAttn(self.embedding_dim)
        
        # paragraph-level encoder
        self.para_encoder = BiLSTMAttn(self.embedding_dim)
        
    def _seg_to_mask(self, segments):
        return (torch.arange(segments.max(), device=segments.device).expand(segments.shape[0], segments.max()) < segments.unsqueeze(1)).float()
    
    # text: [B,w,E], segments: dict(seg_type, [S'])    
    def forward(self, embedded_text, segments): 
        batch_size = embedded_text.shape[0]
        
        # convert to packed tensor
        # [B,w]
        sents_per_doc_mask = self._seg_to_mask(segments['sents_per_doc'])
        # [B,w,E] --> [W,E]
        #print(embedded_text.shape, segments['sents_per_doc'] )
        sentembs = embedded_text.view(-1, self.embedding_dim)[sents_per_doc_mask.view(-1) > 0]
        
        # word-level encoder
#         wl_docembs = self.word_encoder(wordembs, segments['words_per_doc']) # [W,E] --> [B,E]
        
        # sentence-level encoder
#         sl_sentembs = self.word_encoder(wordembs, segments['words_per_sent']) # [W,E] --> [S,E]
#         sl_docembs = self.sent_encoder(sl_sentembs, segments['sents_per_doc']) # [S,E] --> [B,E]
        
        # smash rnn paragraph-level encoder
        #pl_sentembs = self.word_encoder(wordembs, segments['words_per_sent']) # [W,E] --> [S,E]
        pl_paraembs = self.sent_encoder(sentembs, segments['sents_per_para']) # [S,E] --> [P,E]
        pl_docembs = self.para_encoder(pl_paraembs, segments['paras_per_doc']) # [P,E] --> [B,E]
         

        document_embeddings = torch.cat([pl_docembs], dim = 1) # 2 * [B,E] --> [B,2E]
        
        return document_embeddings

class SmashRNN(nn.Module):
    def __init__(self, E, device = DEVICE, dropout = 0.1):
        super().__init__()
        
        #self.vocab_size = V
        self.embedding_dim = E
        self.device = device
        #Smash RNN
        #self.embedder = nn.Embedding(self.vocab_size, self.embedding_dim, padding_idx = pad_idx)
        
        #Smash BERT
        self.embedder = DistilBertModel.from_pretrained(bert_variant)
        

        # freeze the bottom 3 layers, only top 2 layers will be finetuned
        for name, param in self.embedder.named_parameters():
          if "transformer" in name:
            layernum = int(name.split('.')[2])
            param.requires_grad = False if layernum < 5 else True
        self.dummy_layer = DummyLayer()
        self.src_encoder = MashRNN(self.embedding_dim, device)
        self.trg_encoder = MashRNN(self.embedding_dim, device)
        
        self.fc = nn.Linear(2 * self.embedding_dim, self.embedding_dim)
        self.classifier = nn.Linear(self.embedding_dim, 1)
        
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, src_text, src_segs, trg_text, trg_segs):

        #Smash RNN
        #src_wordemb = self.embedder(src_text) # [B,ws] --> [B,ws,E]
        #trg_wordemb = self.embedder(trg_text) # [B,wt] --> [B,wt,E]
        
        #src_wordemb = self.embedder(src_text)[0][:, 0, :]
        #trg_wordemb = self.embedder(trg_text)[0][:, 0, :]


        src_sents = src_text # [S,SL]
        trg_sents = trg_text
        mask_src = (src_sents > 0).float().cuda()
        mask_trg = (trg_sents > 0).float().cuda()

        #print(src_sents.shape, mask_src)
        
        src_sentemb = self.embedder(src_sents, self.dummy_layer(mask_src))[0][:, 0, :] # [S,E]
        #print(src_sentemb.shape)
        trg_sentemb = self.embedder(trg_sents, self.dummy_layer(mask_trg))[0][:, 0, :] # [S,E]
        #docs = U.pad_sequence(torch.split(sents, batch['doc_lens'].tolist()), batch_first=True) # [D,DL,E]
        
        src_docemb = self.src_encoder(src_sentemb, src_segs) # [B,ws,E] --> [B,3E]
        trg_docemb = self.trg_encoder(trg_sentemb, trg_segs) # [B,ws,E] --> [B,3E]
        
        common_embedding = torch.cat([src_docemb, trg_docemb], dim = 1) # 2 * [B,3E] --> [B,6E]
        
        activations = self.dropout(F.relu(self.fc(common_embedding))) # [B,6E] --> [B,E]
        logits = self.dropout(self.classifier(activations)).squeeze(1) # [B,E] --> [B,1] --> [B]
        
        predictions = (torch.sigmoid(logits) > 0.5).int()
        return logits, predictions


TRAINDEV_LEN = len(traindev_num)
DEV_LEN = int(DEV_SPLIT * TRAINDEV_LEN)
TRAIN_LEN = TRAINDEV_LEN - DEV_LEN

train_num = traindev_num[:TRAIN_LEN]

dev_num = traindev_num[TRAIN_LEN:]


"""### Dataset Statistics"""

print("\tDataset Statistics")
print("Train Dataset size:", TRAIN_LEN)
print("Dev Dataset size:", DEV_LEN)

"""## Batching
Shuffle data randomly, convert numericalized batches into padded tensors and split into batches
"""

import random

def batchify(dataset, batch_size = BATCH_SIZE, device = DEVICE):
    #shuffled_dataset = random.sample(dataset, len(dataset))
    
    index = 0
    
    while index < len(dataset):
        sliced = dataset[index : min(index + batch_size, len(dataset))]
        batch = {'src_text': [], 'trg_text': [], 'src_segs': defaultdict(list), 'trg_segs': defaultdict(list), 'labels': []}
        
        for instance in sliced:
            for i in range(instance['src_segs']['sents_per_doc'][0]):
              batch['src_text'].append(torch.tensor(instance['src_text'][i], dtype = torch.long))
            for j in range(instance['trg_segs']['sents_per_doc'][0]):
              batch['trg_text'].append(torch.tensor(instance['trg_text'][j], dtype = torch.long))
            for segname, segbound in instance['src_segs'].items():
                batch['src_segs'][segname].extend(segbound)
            for segname, segbound in instance['trg_segs'].items():
                batch['trg_segs'][segname].extend(segbound)
            batch['labels'].append(instance['label'])
                
        batch['src_text'] = U.pad_sequence(batch['src_text'], batch_first = True).to(device)
        batch['trg_text'] = U.pad_sequence(batch['trg_text'], batch_first = True).to(device)
        for segname, segbound in batch['src_segs'].items():
            batch['src_segs'][segname] = torch.tensor(segbound, dtype = torch.int).to(device)
        for segname, segbound in batch['trg_segs'].items():
            batch['trg_segs'][segname] = torch.tensor(segbound, dtype = torch.int).to(device)
            
        batch['labels'] = torch.tensor(batch['labels'], dtype = torch.float).to(device)
        
#         print(batch['labels'])
            
        yield batch
        
        index += len(batch['labels'])

"""## Define model and optimizer"""

model = SmashRNN(EMBEDDING_DIM, device = DEVICE).to(DEVICE)

optimizer = AdamW(model.parameters(), lr = LR, weight_decay = L2REG)

"""## Evaluate metrics
Calculate Precision, Recall and F1-Score
"""

from sklearn.metrics import precision_score, recall_score, f1_score

def eval_metrics(pred, gold):
    metrics = {}
    metrics['precision'] = precision_score(gold, pred)
    metrics['recall'] = recall_score(gold, pred)
    metrics['f1'] = f1_score(gold, pred)
    return metrics

"""## Training Loop
Run one complete epoch in either train or eval mode. Perform forward (and backward) passes, collect losses and predictions, and evaluate metrics.
"""

import math

def traindev_loop(dataset, batch_size = BATCH_SIZE, train = False):
    num_batches = math.ceil(len(dataset) / batch_size)
    if train:
        model.train()
    else:
        model.eval()
    
    all_pred, all_gold = [], []
    total_loss = 0
    
    skipped = 0
    
#     print(num_batches)
    
    with tqdm(total = num_batches, file = sys.stdout) as pbar:
        if train:
            pbar.set_description("Train:")
        else:
            pbar.set_description("Dev:")
        
        for i, batch in enumerate(batchify(dataset, batch_size=batch_size)):
#             print("%5d"%i, end='\r')
            if DEVICE == 'cuda':
                torch.cuda.empty_cache()
            try:
              logits, pred = model(batch['src_text'], batch['src_segs'], batch['trg_text'], batch['trg_segs'])
              gold = batch['labels']

  #             print(logits.dtype, gold.dtype)
              loss = F.binary_cross_entropy_with_logits(logits, gold)

              if train:
                  optimizer.zero_grad()
                  loss.backward()
                  optimizer.step()

              all_pred.extend(pred.tolist())
              all_gold.extend(gold.tolist())
              total_loss += loss.item()

            except RuntimeError:
                skipped += 1
                continue
            
            finally:
                pass
            pbar.update(1)
            
    metrics = eval_metrics(all_pred, all_gold)
    mean_loss = total_loss / num_batches
    
    return mean_loss, metrics, skipped

"""## Training and Development Phase

### Load saved model
"""

#try:
#model.load_state_dict(torch.load(SAVEPATH + "checkpoint.pt", map_location = DEVICE))
#except FileNotFoundError:
 #   pass

"""### Load best metrics"""

#try:
 #   with open(SAVEPATH + "dev_metrics.json") as fr:
  #      bestmetrics = json.load(fr)
#except FileNotFoundError:
 #   bestmetrics = {'precision': 0, 'recall': 0, 'f1': 0}

#for metric, value in bestmetrics.items():
 #   print(metric, ':', value)

"""### Run epochs"""

bestmetrics = {'f1':0, 'precision' : 0, 'recall' : 0}

for epoch in range(NUM_EPOCHS):
    trainloss, trainmetrics, trainskipped = traindev_loop(train_num, batch_size = BATCH_SIZE, train = True)
    devloss, devmetrics, devskipped = traindev_loop(dev_num, batch_size = BATCH_SIZE, train = False)
    
    print("Epoch: %4d  Tr-Loss: %8.3f  Tr-F1: %5.4f  Tr-Sk: %5d  D-Loss: %8.3f  D-F1: %5.4f  D-Sk: %5d" % (epoch + 1, trainloss, trainmetrics['f1'], trainskipped, devloss, devmetrics['f1'], devskipped))
    
    
    if devmetrics['f1'] > bestmetrics['f1']:
        bestmetrics = devmetrics
        torch.save(model.state_dict(), SAVEPATH + "best_model.pt")
        with open(SAVEPATH + "dev_metrics.json", 'w') as fw:
            json.dump(bestmetrics, fw)
            
    torch.save(model.state_dict(), SAVEPATH + "checkpoint.pt")

