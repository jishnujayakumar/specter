# -*- coding: utf-8 -*-
"""Siamese-Hier-BERT.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AO--T3X0sDMgQB3Rj5NSbO-rA9biBAz6
"""


"""## Imports"""

from tqdm import tqdm
import torch
import torch.nn as nn
import torch.nn.functional as F
import spacy
from collections import defaultdict
import os
import sys
from torch.optim import AdamW
import torch.nn.utils.rnn as U
nlp = spacy.load("en_core_web_sm")
import json

file = "../traindev_jaccard_text_20K_FINAL.jsonl"
#RAWPATH = "data/"
filewrite = "pre-processed-data.json"
NUMPATH = f"{os.environ['ELECTER_HULK_DIR']}/{os.environ['LEGAL_DATA_DIR']}/SHB/{os.environ['SHB_BERT_VARIANT']}"
os.system(f"mkdir -p {NUMPATH}")
OUTPUT_DIR = f"{NUMPATH}/{filewrite}"

"""## Model Hyperparameters"""

from transformers import DistilBertTokenizer
bert_variant = os.environ['SHB_BERT_VARIANT']
bert_tokenizer = DistilBertTokenizer.from_pretrained(bert_variant)

"""## Load dataset
jsonl format source text, target text, label
"""

traindev_raw = []
ct = 0
with open(file) as fr:
   for line in fr:
      #if ct == 5:
       # break
      #ct+=1
      instance = json.loads(line.strip())
      traindev_raw.append(instance)
      
TRAINDEV_LEN = len(traindev_raw)

"""## SpaCy modules for sentences & para splits"""

import spacy
from spacy.attrs import ORTH

nlp = spacy.load("en_core_web_sm")

special_cases = {"Rs.": "Rs.", "No.": "No.", "no.": "No.", "vs.": "vs", "s.": "section", "ss.": "section", "u/s.": "section", "u/ss.": "section", "arts.": "articles", "I.P.C.": "I.P.C.", "Cr.P.C.": "Cr.P.C."}

for tok, orth in special_cases.items():
    nlp.tokenizer.add_special_case(tok, [{ORTH: orth}])

"""## Tokenization
Using BERT tokenizer for sentence to word splits
"""

def tokenize_text_BERT(text):
    return ['[CLS]'] + bert_tokenizer._tokenize(text)[:14] + ['[SEP]']

def preprocess(text):
  parsed = nlp(text)
  cleaned = [tok.lemma_.lower() for tok in parsed if not any([tok.is_stop, tok.is_punct, tok.is_digit, len(tok.lemma_.lower()) == 0])]
  return ' '.join(cleaned)

def segment_doc_bert(doc):
    segments = defaultdict(list)
    wordlist = []
    
    nparas = 0
    for para in doc.strip().split('\n'):
        parsed = nlp(para)
        
        nsents = 0
        for sent in parsed.sents:
            nwords = 0
            sent = preprocess(sent.text)
            #for tok in sent:
                #if any([tok.is_punct, tok.is_stop, tok.is_digit]):
                    #continue
            bert_toks = tokenize_text_BERT(sent)
                #wordlist.append(tok.lemma_.lower())
            nwords = len(bert_toks)
                
            if nwords > 0:
                nsents += 1
                wordlist.append(bert_toks)
              #  segments['words_per_sent'].append(nwords)
        
        if nsents > 0:
            nparas += 1
            segments['sents_per_para'].append(nsents)
            
    if nparas > 0:
        segments['paras_per_doc'].append(nparas)
        
    #segments['words_per_doc'].append(len(wordlist))
    segments['sents_per_doc'].append(sum(segments['sents_per_para']))
    
    #assert len(wordlist) == sum(segments['words_per_sent']), "Error in segmenting words"
    #assert len(segments['words_per_sent']) == sum(segments['sents_per_para']), "Error in segmenting sentences"
    assert len(segments['sents_per_para']) == sum(segments['paras_per_doc']), "Error in segmenting paragraphs"
    
    return wordlist, segments

traindev_tok_bert = []

with tqdm(total = TRAINDEV_LEN, file = sys.stdout) as pbar:
    pbar.set_description("Tokenizing train dev data:")
    
    for instance_raw in traindev_raw:
        instance_tok = {}
  
        instance_tok['src_text'], instance_tok['src_segs'] = segment_doc_bert(instance_raw['srctext'])
        instance_tok['trg_text'], instance_tok['trg_segs'] = segment_doc_bert(instance_raw['trgtext'])
        instance_tok['label'] = instance_raw['lab']
        
        traindev_tok_bert.append(instance_tok)

        pbar.update(1)

"""## Numericalization"""

def numericalize(tokens):
    return list(map(lambda x: bert_tokenizer._convert_token_to_id(x), tokens))

traindev_num = []
print("\n\n *************** NUMERICALIZING ******************** ")
for instance_tok in traindev_tok_bert:
    instance_num = {}
    
    instance_num['src_text'] = list(map(lambda x: numericalize(x), instance_tok['src_text']))
    instance_num['trg_text'] = list(map(lambda x: numericalize(x), instance_tok['trg_text']))
    instance_num['src_segs'] = instance_tok['src_segs']
    instance_num['trg_segs'] = instance_tok['trg_segs']
    instance_num['label'] = instance_tok['label']
    
    traindev_num.append(instance_num)

with open(OUTPUT_DIR, 'w') as fw:
    json.dump(traindev_num, fw)
